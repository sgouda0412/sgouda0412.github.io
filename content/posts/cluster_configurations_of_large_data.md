+++
title = 'ğŸš€ What 99% of PySpark Users Get Wrong About Processing Large Files (500GB-1TB)'
date = 2025-04-02T10:03:26+05:30
draft = false
tags = ['pyspark']
author = 'Santosh Kumar Gouda'
+++

## ğŸ“‘ Table of Contents
- [ğŸ” Introduction](#-introduction)
- [âš ï¸ Understanding the Challenges of Large-Scale Data Processing](#ï¸-understanding-the-challenges-of-large-scale-data-processing)
  - [ğŸ’¾ Memory Limitations](#-memory-limitations)
  - [ğŸ’½ Disk I/O Bottlenecks](#-disk-io-bottlenecks)
  - [ğŸŒ Network Overhead](#-network-overhead)
  - [ğŸ§© Partitioning Issues](#-partitioning-issues)
- [âš™ï¸ Cluster Configuration for Massive Datasets](#ï¸-cluster-configuration-for-massive-datasets)
  - [ğŸ–¥ï¸ Executor Memory & Cores](#ï¸-executor-memory--cores)
  - [ğŸ® Driver Memory Settings](#-driver-memory-settings)
  - [âš–ï¸ Dynamic vs. Static Allocation](#ï¸-dynamic-vs-static-allocation)
  - [ğŸ”¢ Parallelism & Partition Tuning](#-parallelism--partition-tuning)
- [ğŸ“Š Optimal File Formats for Big Data](#-optimal-file-formats-for-big-data)
  - [ğŸ“ CSV vs. Parquet vs. ORC vs. Avro](#-csv-vs-parquet-vs-orc-vs-avro)
  - [ğŸ—œï¸ Compression Techniques](#ï¸-compression-techniques)
  - [âœ‚ï¸ Splittable vs. Non-Splittable Files](#ï¸-splittable-vs-non-splittable-files)
- [ğŸ“– Reading Large Files Efficiently](#-reading-large-files-efficiently)
  - [ğŸ§© Partitioned Reads](#-partitioned-reads)
  - [ğŸ” Predicate Pushdown & Column Pruning](#-predicate-pushdown--column-pruning)
  - [ğŸ§® Chunked Processing](#-chunked-processing)
- [ğŸ§  Memory Management & Optimization](#-memory-management--optimization)
  - [ğŸ’¾ Spill-to-Disk Strategies](#-spill-to-disk-strategies)
  - [ğŸ’« Caching & Persistence Levels](#-caching--persistence-levels)
  - [ğŸ—‘ï¸ Garbage Collection Tuning](#ï¸-garbage-collection-tuning)
- [ğŸ”„ Handling Joins, Aggregations, and Shuffles](#-handling-joins-aggregations-and-shuffles)
  - [ğŸ“¡ Broadcast Joins for Small Tables](#-broadcast-joins-for-small-tables)
  - [ğŸ§‚ Salting for Skewed Data](#-salting-for-skewed-data)
  - [ğŸš« Avoiding Full Shuffles](#-avoiding-full-shuffles)
- [ğŸ“Š Monitoring & Debugging Performance](#-monitoring--debugging-performance)
  - [ğŸ” Spark UI Deep Dive](#-spark-ui-deep-dive)
  - [ğŸ“ Log Analysis & Metrics](#-log-analysis--metrics)
  - [ğŸ’¥ Handling OOM Errors](#-handling-oom-errors)
- [ğŸ’¾ Writing Large Outputs Efficiently](#-writing-large-outputs-efficiently)
  - [ğŸ“‚ Partitioned Writes](#-partitioned-writes)
  - [ğŸ“„ Avoiding Small Files Problem](#-avoiding-small-files-problem)
  - [ğŸ”„ Coalesce vs. Repartition](#-coalesce-vs-repartition)
- [ğŸ“š Real-World Case Studies](#-real-world-case-studies)
  - [ğŸ“Š Processing 1TB CSV Files](#-processing-1tb-csv-files)
  - [âš¡ Optimizing a 500GB Parquet Dataset](#-optimizing-a-500gb-parquet-dataset)
- [ğŸ’¡ Best Practices & Pro Tips](#-best-practices--pro-tips)
- [ğŸ Conclusion](#-conclusion)

## ğŸ” Introduction

Processing files ranging from 100GB to 1TB in PySpark is a common challenge in big data pipelines. If not handled correctly, it can lead to:

- âŒ Out-of-Memory (OOM) crashes
- â±ï¸ Slow processing due to excessive shuffles
- ğŸ’½ Disk I/O bottlenecks
- ğŸŒ‹ Cluster instability

This guide provides every minute detail on how to read, process, and write massive datasets efficiently in PySpark without breaking your cluster.

## âš ï¸ Understanding the Challenges of Large-Scale Data Processing

### ğŸ’¾ Memory Limitations
- Spark loads data into executor memory before processing.
- If the dataset is larger than available memory, it spills to disk, slowing down jobs.
- **Fix**: Increase `spark.executor.memory` and optimize partitioning.

### ğŸ’½ Disk I/O Bottlenecks
- Reading/writing 1TB from HDD (vs. SSD) can take hours.
- **Fix**: Use columnar formats (Parquet/ORC) and distributed storage (S3, HDFS).

### ğŸŒ Network Overhead
- Shuffling 1TB across nodes can cause network congestion.
- **Fix**: Minimize shuffles with broadcast joins and partitioning.

### ğŸ§© Partitioning Issues
- Too few partitions â†’ underutilized cluster.
- Too many partitions â†’ overhead in task scheduling.
- **Fix**: Aim for 128MBâ€“256MB per partition.

## âš™ï¸ Cluster Configuration for Massive Datasets

### ğŸ–¥ï¸ Executor Memory & Cores

```python
spark = SparkSession.builder \
    .config("spark.executor.memory", "8g") \  # 8GB per executor
    .config("spark.executor.cores", "4") \   # 4 cores per executor
    .config("spark.executor.instances", "20") \  # 20 executors
    .getOrCreate()
```

**Rule of thumb**:
- Executor Memory = 4GBâ€“16GB (avoid going beyond 64GB due to GC overhead).
- Cores per Executor = 4â€“8 (more leads to contention).

### ğŸ® Driver Memory Settings

The driver manages task scheduling and metadata.
For 1TB datasets, set:

```python
.config("spark.driver.memory", "8g")  
```

If collecting data to driver (e.g., `.collect()`), increase further.

### âš–ï¸ Dynamic vs. Static Allocation

- **Static Allocation**: Fixed resources (good for predictable workloads).
- **Dynamic Allocation**: Scales executors based on load.

```python
.config("spark.dynamicAllocation.enabled", "true")  
.config("spark.shuffle.service.enabled", "true")  
```

### ğŸ”¢ Parallelism & Partition Tuning

Default partitions = 200, but for 1TB data:

```python
spark.conf.set("spark.sql.shuffle.partitions", "1000")  
```

Adjust partition size:

```python
spark.conf.set("spark.sql.files.maxPartitionBytes", "256MB")  
```

## ğŸ“Š Optimal File Formats for Big Data

| Format  | Splittable? | Compression | Best For                   |
|---------|-------------|-------------|----------------------------|
| CSV     | âŒ No       | Low         | Simple, human-readable     |
| JSON    | âŒ No       | Low         | Nested data                |
| Parquet | âœ… Yes      | High (Snappy/Zstd) | Analytics, columnar queries |
| ORC     | âœ… Yes      | High        | Hive optimizations         |
| Avro    | âœ… Yes      | Medium      | Row-based storage          |

Best choice for 1TB data:

```python
df = spark.read.parquet("s3://path/to/data.parquet")  
```

### ğŸ“ CSV vs. Parquet vs. ORC vs. Avro

- **CSV**: Human-readable but inefficient for large data
- **Parquet**: Column-oriented, ideal for analytics queries
- **ORC**: Similar to Parquet but optimized for Hive
- **Avro**: Great for row-based processing with schema evolution

### ğŸ—œï¸ Compression Techniques

- **Snappy**: Good balance of speed and compression (default for Parquet)
- **Gzip**: Higher compression ratio but slower processing
- **Zstd**: Better than Snappy in both compression and speed

### âœ‚ï¸ Splittable vs. Non-Splittable Files

- **Splittable**: Can be processed in parallel chunks (Parquet, ORC, Avro)
- **Non-Splittable**: Must be read as a single file (CSV, JSON with compression)

## ğŸ“– Reading Large Files Efficiently

### ğŸ§© Partitioned Reads

```python
df = spark.read \
    .option("partitionSize", "256MB") \  # Control split size
    .csv("s3://big-file.csv")
```

### ğŸ” Predicate Pushdown & Column Pruning

Only read needed columns:

```python
df.select("column1", "column2")  
```

Filter early:

```python
df.filter(df["date"] > "2023-01-01")  
```

### ğŸ§® Chunked Processing

Useful for non-splittable formats (CSV/JSON).
Example:

```python
for i in range(0, file_size, chunk_size):
    chunk = spark.read \
        .option("range", f"{i}GB-{i+chunk_size}GB") \
        .csv("s3://big-file.csv")
    process(chunk)
```

## ğŸ§  Memory Management & Optimization

### ğŸ’¾ Spill-to-Disk Strategies

If memory fills up, Spark spills to disk.
Tuning:

```python
.config("spark.memory.fraction", "0.8")  # 80% for execution/storage
.config("spark.memory.storageFraction", "0.5")  # 50% reserved for storage
```

### ğŸ’« Caching & Persistence Levels

Cache wisely:

```python
df.persist(StorageLevel.MEMORY_AND_DISK)  # Spill to disk if OOM
```

Unpersist when done:

```python
df.unpersist()
```

### ğŸ—‘ï¸ Garbage Collection Tuning

Use G1GC for large heaps:

```python
.config("spark.executor.extraJavaOptions", "-XX:+UseG1GC")  
```

## ğŸ”„ Handling Joins, Aggregations, and Shuffles

### ğŸ“¡ Broadcast Joins for Small Tables

```python
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")  
df_large.join(broadcast(df_small), "key")  
```

### ğŸ§‚ Salting for Skewed Data

Add a random salt to distribute skewed keys:

```python
from pyspark.sql.functions import rand
df = df.withColumn("salt", (rand() * 10).cast("int"))  
```

### ğŸš« Avoiding Full Shuffles

- Use repartition before joins/aggregations.
- Bucketing (for frequently joined columns):

```python
df.write.bucketBy(100, "user_id").saveAsTable("bucketed_table")  
```

## ğŸ“Š Monitoring & Debugging Performance

### ğŸ” Spark UI Deep Dive

Check:
- Stages with long GC time â†’ Increase memory.
- Skewed partitions â†’ Use salting/repartitioning.
- Spill to disk â†’ Increase spark.memory.fraction.

### ğŸ“ Log Analysis & Metrics

```python
spark.sparkContext.setLogLevel("INFO")  # DEBUG for detailed logs
```

### ğŸ’¥ Handling OOM Errors

**Symptoms**: `java.lang.OutOfMemoryError`

**Solutions**:
- Increase `spark.executor.memory`.
- Use `.repartition()`.
- Avoid `.collect()`.

## ğŸ’¾ Writing Large Outputs Efficiently

### ğŸ“‚ Partitioned Writes

```python
df.write.partitionBy("date").parquet("s3://output/")  
```

### ğŸ“„ Avoiding Small Files Problem

**Coalesce**: Reduces partitions (may cause skew).

```python
df.coalesce(100).write.parquet("s3://output/")  
```

**Repartition**: Evenly distributes data.

```python
df.repartition(100).write.parquet("s3://output/")  
```

### ğŸ”„ Coalesce vs. Repartition

- **Coalesce**: Combines partitions without full shuffle (faster but may be unbalanced)
- **Repartition**: Full shuffle to ensure even distribution (slower but balanced)

## ğŸ“š Real-World Case Studies

### ğŸ“Š Processing 1TB CSV Files

**Problem**: Slow reads, OOM errors.

**Solution**:
1. Convert to Parquet first.
2. Use 256MB partitions.
3. Filter early.

### âš¡ Optimizing a 500GB Parquet Dataset

**Problem**: Skewed joins.

**Solution**:
1. Broadcast small tables.
2. Salt skewed keys.

## ğŸ’¡ Best Practices & Pro Tips

- âœ… Always test on a sample first.
- âœ… Avoid `.collect()` on large datasets.
- âœ… Use structured streaming for incremental processing.
- âœ… Monitor Spark UI for bottlenecks.
- âœ… Consider using Delta Lake for ACID transactions on large datasets.
- âœ… Set appropriate checkpoint intervals for long-running jobs.
- âœ… Use window functions instead of joins when possible.

## ğŸ Conclusion

Processing 500GBâ€“1TB files in PySpark requires:
- âœ”ï¸ Proper cluster configuration
- âœ”ï¸ Efficient file formats (Parquet/ORC)
- âœ”ï¸ Optimal partitioning & parallelism
- âœ”ï¸ Minimizing shuffles & memory spills

